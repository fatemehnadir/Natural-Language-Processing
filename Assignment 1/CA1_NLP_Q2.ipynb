{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb31422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import itertools\n",
    "\n",
    "from collections.abc import Sequence\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "647952a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = './data/'\n",
    "\n",
    "with open(file_dir + 'hp_fa.txt', 'r') as f:\n",
    "    hp_fa = f.read()\n",
    "    \n",
    "with open(file_dir + 'hp_en.txt', 'r') as f:\n",
    "    hp_en = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2841c0",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d61b6f",
   "metadata": {},
   "source": [
    "## White Space Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4defea42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of extracted tokens(White Space Tokenization): English harry potter book 78443\n",
      "------------------------------------------------------------------------------------------\n",
      "The number of extracted tokens(White Space Tokenization): Persain harry potter book 96294\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "     \n",
    "# Create a reference variable for Class WhitespaceTokenizer\n",
    "tk = WhitespaceTokenizer()\n",
    "     \n",
    "# Use tokenize method\n",
    "hp_en_tokenized = tk.tokenize(hp_en)\n",
    "hp_fa_tokenized = tk.tokenize(hp_fa)\n",
    "\n",
    "print(\"The number of extracted tokens(White Space Tokenization): English harry potter book\", len(hp_en_tokenized))\n",
    "print(f\"------------------------------------------------------------------------------------------\")\n",
    "print(\"The number of extracted tokens(White Space Tokenization): Persain harry potter book\", len(hp_fa_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1944b289",
   "metadata": {},
   "source": [
    "## Spacy Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49dd9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of extracted tokens(Spacy Tokenizer): English harry potter book 102406\n",
      "----------------------------------------------------------------------------------\n",
      "The number of extracted tokens(Spacy Tokenizer): Persian harry potter book 125677\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.fa import Persian\n",
    "\n",
    "nlp_English = English()\n",
    "tokens = nlp_English(hp_en)\n",
    "\n",
    "hp_en_tokenized = []\n",
    "for token in tokens:\n",
    "    hp_en_tokenized.append(token)\n",
    "    \n",
    "print(\"The number of extracted tokens(Spacy Tokenizer): English harry potter book\", len(hp_en_tokenized))\n",
    "\n",
    "print(f\"----------------------------------------------------------------------------------\")\n",
    "nlp_Persian = Persian()\n",
    "tokens = nlp_Persian(hp_fa)\n",
    "\n",
    "hp_fa_tokenized = []\n",
    "for token in tokens:\n",
    "    hp_fa_tokenized.append(token)\n",
    "    \n",
    "print(\"The number of extracted tokens(Spacy Tokenizer): Persian harry potter book\", len(hp_fa_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b97235",
   "metadata": {},
   "source": [
    "## (BPE) Subword Tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c0a6b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The number of extracted tokens(BPE Tokenizer): English harry potter book 100012\n",
      "--------------------------------------------------------------------------------\n",
      "The number of extracted tokens(BPE Tokenizer): Persian harry potter book 100932\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "\n",
    "# Initialize a trainer with the BPE model\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"])\n",
    "\n",
    "# Train the BPE model on your sentence\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "\n",
    "\n",
    "tokenizer.train_from_iterator([hp_en], trainer)\n",
    "hp_en_tokenized = tokenizer.encode(hp_en)\n",
    "\n",
    "\n",
    "tokenizer.train_from_iterator([hp_fa], trainer)\n",
    "hp_fa_tokenized = tokenizer.encode(hp_fa)\n",
    "\n",
    "print(\"The number of extracted tokens(BPE Tokenizer): English harry potter book\", len(hp_en_tokenized.tokens))\n",
    "print(f\"--------------------------------------------------------------------------------\")\n",
    "print(\"The number of extracted tokens(BPE Tokenizer): Persian harry potter book\", len(hp_fa_tokenized.tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d892893",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c1271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English string\n",
    "sen_en = \"This question is about tokenization and shows several tokenizer algorithms.Hopefully, you\\nwill be able to understand how they are trained and generate tokens.\"\n",
    "     \n",
    "# Persian string\n",
    "sen_fa = \"این سوال در مورد قطعه بندی جملات است و چندین الگوریتم توکنایز کردن متن را نشان می دهد. امیدواریم بتوانید نحوه آموزش آنها و تولید توکن ها را درک کنید.\"\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ecce4",
   "metadata": {},
   "source": [
    "## White Space Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3db28c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of extracted tokens(White Space Tokenization): English sentence 23\n",
      "------------------------------------------------------------------------------------------\n",
      "The number of extracted tokens(White Space Tokenization): Persain sentence 30\n"
     ]
    }
   ],
   "source": [
    "sen_en_tokenized = tk.tokenize(sen_en)\n",
    "sen_fa_tokenized = tk.tokenize(sen_fa)\n",
    "\n",
    "print(\"The number of extracted tokens(White Space Tokenization): English sentence\", len(sen_en_tokenized))\n",
    "print(f\"------------------------------------------------------------------------------------------\")\n",
    "print(\"The number of extracted tokens(White Space Tokenization): Persain sentence\", len(sen_fa_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35c89fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This, question, is, about, tokenization, and, shows, several, tokenizer, algorithms.Hopefully,, you, will, be, able, to, understand, how, they, are, trained, and, generate, tokens.\n"
     ]
    }
   ],
   "source": [
    "print(*sen_en_tokenized, sep = \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53268e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "این, سوال, در, مورد, قطعه, بندی, جملات, است, و, چندین, الگوریتم, توکنایز, کردن, متن, را, نشان, می, دهد., امیدواریم, بتوانید, نحوه, آموزش, آنها, و, تولید, توکن, ها, را, درک, کنید.\n"
     ]
    }
   ],
   "source": [
    "print(*sen_fa_tokenized, sep = \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c3656",
   "metadata": {},
   "source": [
    "## Spacy Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4df5958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of extracted tokens(White Space Tokenization): English sentence 28\n",
      "----------------------------------------------------------------------------------\n",
      "The number of extracted tokens(Spacy Tokenizer): Persian harry potter book 33\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp_English(sen_en)\n",
    "sen_en_tokenized = []\n",
    "for token in tokens:\n",
    "    sen_en_tokenized.append(token)\n",
    "    \n",
    "\n",
    "tokens = nlp_Persian(sen_fa)\n",
    "\n",
    "sen_fa_tokenized = []\n",
    "for token in tokens:\n",
    "    sen_fa_tokenized.append(token)\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"The number of extracted tokens(White Space Tokenization): English sentence\", len(sen_en_tokenized))\n",
    "print(f\"----------------------------------------------------------------------------------\")\n",
    "print(\"The number of extracted tokens(Spacy Tokenizer): Persian harry potter book\", len(sen_fa_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b9a6ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This, question, is, about, tokenization, and, shows, several, tokenizer, algorithms, ., Hopefully, ,, you, \n",
      ", will, be, able, to, understand, how, they, are, trained, and, generate, tokens, .\n"
     ]
    }
   ],
   "source": [
    "print(*sen_en_tokenized, sep = \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df1a8f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "این, سوال, در, مورد, قطعه, بندی, جملات, است, و, چندین, الگوریتم, توکنایز, کردن, متن, را, نشان, می, دهد, ., امیدوار, یم, بتوانید, نحوه, آموزش, آنها, و, تولید, توکن, ها, را, درک, کنید, .\n"
     ]
    }
   ],
   "source": [
    "print(*sen_fa_tokenized, sep = \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba41e1",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c83396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The number of extracted tokens(BPE Tokenizer): English sentence 42\n",
      "--------------------------------------------------------------------------------\n",
      "The number of extracted tokens(BPE Tokenizer): Persian sentence 63\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator([hp_en], trainer)\n",
    "sen_en_tokenized = tokenizer.encode(sen_en)\n",
    "\n",
    "tokenizer.train_from_iterator([hp_fa], trainer)\n",
    "sen_fa_tokenized = tokenizer.encode(sen_fa)\n",
    "\n",
    "print(\"The number of extracted tokens(BPE Tokenizer): English sentence\", len(sen_en_tokenized.tokens))\n",
    "print(f\"--------------------------------------------------------------------------------\")\n",
    "print(\"The number of extracted tokens(BPE Tokenizer): Persian sentence\", len(sen_fa_tokenized.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64702195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This, question, is, about, to, ken, i, z, ation, and, shows, several, to, ken, i, z, er, al, gor, ith, ms, ., Hopefully, ,, you, will, be, able, to, understand, how, they, are, trained, and, g, en, er, ate, to, kens, .\n"
     ]
    }
   ],
   "source": [
    "l = list(sen_en_tokenized.tokens)\n",
    "print(*l, sep = \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "983a2a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ا, <unk>, ن, سو, ال, در, مورد, قط, عه, بند, <unk>, جم, لات, است, و, چند, <unk>, ن, ال, گور, <unk>, تم, تو, کن, ا, <unk>, ز, کردن, متن, را, نشان, م, <unk>, دهد, A, ام, <unk>, دو, ار, <unk>, م, ب, توان, <unk>, د, ن, حو, ه, آموزش, آنها, و, تول, <unk>, د, تو, کن, ها, را, درک, کن, <unk>, د, A\n"
     ]
    }
   ],
   "source": [
    "l = list(sen_fa_tokenized.tokens)\n",
    "print(*l, sep = \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b661727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed5fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
