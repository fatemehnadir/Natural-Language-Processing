{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851686f8",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9cad8",
   "metadata": {},
   "source": [
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44f836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/fatemehnadi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/fatemehnadi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install -U nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f74c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#from imblearn.over_sampling import  SMOTE\n",
    "\n",
    "\n",
    "import random \n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import tqdm\n",
    "import re\n",
    "\n",
    "from nltk import FreqDist\n",
    "import itertools\n",
    "\n",
    "from collections.abc import Sequence\n",
    "#from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "#from nltk.lm import MLE\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from nltk import word_tokenize\n",
    "#from nltk.lm import MLE, Laplace\n",
    "#from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3f2d0",
   "metadata": {},
   "source": [
    "### read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f724bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>واقعا حیف وقت که بنویسم سرویس دهیتون شده افتضاح</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>قرار بود ۱ ساعته برسه ولی نیم ساعت زودتر از مو...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>قیمت این مدل اصلا با کیفیتش سازگاری نداره، فقط...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>عالللی بود همه چه درست و به اندازه و کیفیت خوب...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>شیرینی وانیلی فقط یک مدل بود.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label_id\n",
       "0    واقعا حیف وقت که بنویسم سرویس دهیتون شده افتضاح         1\n",
       "1  قرار بود ۱ ساعته برسه ولی نیم ساعت زودتر از مو...         0\n",
       "2  قیمت این مدل اصلا با کیفیتش سازگاری نداره، فقط...         1\n",
       "3  عالللی بود همه چه درست و به اندازه و کیفیت خوب...         0\n",
       "4                      شیرینی وانیلی فقط یک مدل بود.         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./Snappfood - Sentiment Analysis.csv\" , on_bad_lines='skip' , delimiter='\\t')\n",
    "\n",
    "data = data[['comment', 'label_id']] # remove Unamed column\n",
    "data = data.dropna()                            # drop nan\n",
    "data['label_id'] = data['label_id'].astype(int) # convert to int\n",
    "\n",
    "# show data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d389074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "0    34916\n",
       "1    34564\n",
       "Name: label_id, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['label_id'])['label_id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dc06b2",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d748b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_balanced(data, sample_size=0.1):\n",
    "    \n",
    "    # shuffle the DataFrame rows & return all rows\n",
    "    data = data.sample(frac = 1)\n",
    "    data.reset_index(drop=True, inplace=True) \n",
    "    \n",
    "    data_pos = data[data['label_id']==0]\n",
    "    data_neg = data[data['label_id']==1]\n",
    "    \n",
    "    no_pos = int(np.floor((data.groupby(['label_id'])['label_id'].count()[0] * sample_size)))\n",
    "    no_neg = int(np.floor((data.groupby(['label_id'])['label_id'].count()[1] * sample_size)))\n",
    "    \n",
    "    \n",
    "    data_pos_train = data_pos[:no_pos]\n",
    "    data_neg_train = data_neg[:no_neg]\n",
    "    \n",
    "    \n",
    "    data_pos_test = data_pos[no_pos+1:]\n",
    "    data_neg_test = data_neg[no_neg+1:]\n",
    "    \n",
    "    # train\n",
    "    df_train = pd.concat([data_pos_train, data_neg_train])\n",
    "    df_train.reset_index(drop=True, inplace=True) # reset index\n",
    "    \n",
    "    \n",
    "    # test\n",
    "    df_test = pd.concat([data_pos_test, data_neg_test])\n",
    "    df_test.reset_index(drop=True, inplace=True) # reset index\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_train, df_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25d7f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data, _  = sampling_balanced(data,sample_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008232c0",
   "metadata": {},
   "source": [
    "### preprocess persian text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a90889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hazm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7bc54dd",
   "metadata": {},
   "source": [
    "Load stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f72a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_list(stopwords_file):\n",
    "    with open(stopwords_file, 'r', encoding='utf-8') as file:\n",
    "        stopwords = file.read().split('\\n')\n",
    "    return stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aaaff62",
   "metadata": {},
   "source": [
    "Remove and change some characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd85c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_chars(comment):\n",
    "    comment = re.sub(r'[^\\w\\s]', ' ', comment) \n",
    "    comment = re.sub(r'\\d+', ' ', comment)  \n",
    "    comment = re.sub(r'[a-zA-Z]', ' ', comment)\n",
    "    comment = re.sub(r'[0-9]+', '', comment)\n",
    "    comment = re.sub(r'[يى]', 'ی', comment) \n",
    "    comment = re.sub(r'[ك]', 'ک', comment) \n",
    "    comment = re.sub(r'[گ]', 'گ', comment)  \n",
    "    comment = re.sub(r'\\s+', ' ', comment) \n",
    "    #comment = re.sub(r'[۰-۹a-zA-Z]+', '', comment)  # remove extra spaces\n",
    "    return comment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02a15afb",
   "metadata": {},
   "source": [
    "Preprocess a single comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e6baac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_comment(comment):\n",
    "    \n",
    "    norm_comment = replace_chars(comment)\n",
    "    \n",
    "    # num_removed = remove_numbers(norm_comment)\n",
    "    # Normalize the comment\n",
    "    normalizer = Normalizer()\n",
    "    norm_comment = normalizer.normalize(norm_comment)\n",
    "    words = word_tokenize(norm_comment)\n",
    "    stop_words = stopwords_list(\"./short_stop_words.txt\")\n",
    "    '''\n",
    "    my_stopwords = ['هم', 'که','آن', 'را' , 'برای','مثلا']\n",
    "    default_stopwords = hazm.stopwords_list() + my_stopwords\n",
    "    \n",
    "    my_stopwords = ['عالی', 'بد', 'زیادی', 'بهتر', 'همه', 'باعث', 'معمولی', 'خیلی','خوب', 'یک','بی','ی']\n",
    "    stopwords = [word for word in default_stopwords if word not in my_stopwords]\n",
    "\n",
    "    '''\n",
    "\n",
    "    filtered_words = [word for word in words if word not in stop_words and not bool(re.search('[\\W_]', word))]\n",
    "\n",
    "    lemmatizer = Lemmatizer()\n",
    "    farsi_words = []\n",
    "    for word in filtered_words:\n",
    "        if not bool(re.search('[a-zA-Z]', word)):\n",
    "            farsi_words.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "    return ' '.join(farsi_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4039db0f",
   "metadata": {},
   "source": [
    "Apply preprocess on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2121073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "\n",
    "'''# Tokenize words\n",
    "def tokenize_words(comment):\n",
    "    return word_tokenize(comment)'''\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess a single comment\n",
    "\n",
    "\n",
    "# Preprocess a list of comments\n",
    "def preprocess_comments(comments):\n",
    "    preprocessed_comments = []\n",
    "    for comment in comments:\n",
    "        preprocessed_comment = preprocess_comment(comment)\n",
    "        preprocessed_comments.append(preprocessed_comment)\n",
    "    return preprocessed_comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36b50e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comments = sample_data['comment'].tolist()\n",
    "labels = sample_data['label_id'].tolist()\n",
    "preprocessed_comments = preprocess_comments(comments)\n",
    "preprocessed_df = pd.DataFrame({'comment': preprocessed_comments, 'label_id': labels})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a59736",
   "metadata": {},
   "source": [
    "split dataset to test and train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f286e",
   "metadata": {},
   "source": [
    "## Part1 :  TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3a1fb",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fc22bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    \n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "        \n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faccbea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TF(df):\n",
    "\n",
    "    bagOfw = df.comment.apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)\n",
    "    bagOfw\n",
    "\n",
    "    uniqueWords =  set((bagOfw.index).to_list())\n",
    "    print(\"No. of uniqueWords: \",len(uniqueWords))\n",
    "\n",
    "    numOfWords_list = [] \n",
    "    bagOfWords_list = []\n",
    "\n",
    "    \n",
    "    tf_list = [] # The frequency of each word in each document is stored in a dictionary\n",
    "\n",
    "    for i in range(0,df.shape[0]):\n",
    "\n",
    "        bagOfWords = str(df.iloc[i]['comment']).split(' ')\n",
    "\n",
    "        numOfWords = dict.fromkeys(uniqueWords, 0)\n",
    "        for word in bagOfWords:\n",
    "            numOfWords[word] += 1\n",
    "\n",
    "        tf = computeTF(numOfWords, bagOfWords)\n",
    "\n",
    "        bagOfWords_list.append(bagOfWords)\n",
    "        numOfWords_list.append(numOfWords)\n",
    "        tf_list.append(tf)\n",
    "\n",
    "    return tf_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c790893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of uniqueWords:  6255\n"
     ]
    }
   ],
   "source": [
    "tf_list = get_TF(preprocessed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a8e39",
   "metadata": {},
   "source": [
    "### IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "318e0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "725f37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF(tf_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49551c04",
   "metadata": {},
   "source": [
    "### Tf_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf864d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57cc249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_list = []\n",
    "\n",
    "for i in range(0,preprocessed_df.shape[0]):\n",
    "    tfidf = computeTFIDF(tf_list[i], idfs)\n",
    "    tfidf_list.append(tfidf)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f2a0c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label_id</th>\n",
       "      <th></th>\n",
       "      <th>تضمین</th>\n",
       "      <th>ساخت</th>\n",
       "      <th>کلاخمیربود</th>\n",
       "      <th>صداقتتون</th>\n",
       "      <th>خورمش</th>\n",
       "      <th>بیکران</th>\n",
       "      <th>نارنجک</th>\n",
       "      <th>...</th>\n",
       "      <th>چهارتادونه</th>\n",
       "      <th>ممنونممم</th>\n",
       "      <th>دونفرسیرمیکنه</th>\n",
       "      <th>خشکه</th>\n",
       "      <th>بیکن</th>\n",
       "      <th>ناکافی</th>\n",
       "      <th>زغال</th>\n",
       "      <th>انجا</th>\n",
       "      <th>پرانتز</th>\n",
       "      <th>قیمتا</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>تاخیر خییییییلی زیاد … خوشمزه بود</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>بنده در عرض پنج دقیقه دو تا سفارش از هایپر میو...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>بسیار عالی، بسته بندی خوب و حرفه‌ای، تنوع غذای...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>کیفیت غذا قبلا بهتر بود</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>خوب بود فقط خیلی قطور بود که باعث میشد دل آدم ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label_id       تضمین   \n",
       "0                  تاخیر خییییییلی زیاد … خوشمزه بود         0  0.0    0.0  \\\n",
       "1  بنده در عرض پنج دقیقه دو تا سفارش از هایپر میو...         0  0.0    0.0   \n",
       "2  بسیار عالی، بسته بندی خوب و حرفه‌ای، تنوع غذای...         0  0.0    0.0   \n",
       "3                            کیفیت غذا قبلا بهتر بود         0  0.0    0.0   \n",
       "4  خوب بود فقط خیلی قطور بود که باعث میشد دل آدم ...         0  0.0    0.0   \n",
       "\n",
       "   ساخت  کلاخمیربود  صداقتتون  خورمش  بیکران  نارنجک  ...  چهارتادونه   \n",
       "0   0.0         0.0       0.0    0.0     0.0     0.0  ...         0.0  \\\n",
       "1   0.0         0.0       0.0    0.0     0.0     0.0  ...         0.0   \n",
       "2   0.0         0.0       0.0    0.0     0.0     0.0  ...         0.0   \n",
       "3   0.0         0.0       0.0    0.0     0.0     0.0  ...         0.0   \n",
       "4   0.0         0.0       0.0    0.0     0.0     0.0  ...         0.0   \n",
       "\n",
       "   ممنونممم  دونفرسیرمیکنه  خشکه  بیکن  ناکافی  زغال  انجا  پرانتز  قیمتا  \n",
       "0       0.0            0.0   0.0   0.0     0.0   0.0   0.0     0.0    0.0  \n",
       "1       0.0            0.0   0.0   0.0     0.0   0.0   0.0     0.0    0.0  \n",
       "2       0.0            0.0   0.0   0.0     0.0   0.0   0.0     0.0    0.0  \n",
       "3       0.0            0.0   0.0   0.0     0.0   0.0   0.0     0.0    0.0  \n",
       "4       0.0            0.0   0.0   0.0     0.0   0.0   0.0     0.0    0.0  \n",
       "\n",
       "[5 rows x 6257 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_TFIDF = pd.concat([sample_data, tfidf_df], axis=1)\n",
    "final_TFIDF.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75f4909b",
   "metadata": {},
   "source": [
    "split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba34f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_df\n",
    "y =  preprocessed_df['label_id']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27097627",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier\n",
    "\n",
    "calculate F1 score, precision, recall, and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57209874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.82      0.67       345\n",
      "           1       0.68      0.38      0.49       350\n",
      "\n",
      "    accuracy                           0.60       695\n",
      "   macro avg       0.62      0.60      0.58       695\n",
      "weighted avg       0.62      0.60      0.58       695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229ac7b",
   "metadata": {},
   "source": [
    "## Part 2 : PPMI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3644ac50",
   "metadata": {},
   "source": [
    "remove empty comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a53a8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = preprocessed_df[preprocessed_df['comment'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ee07cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppmi(df):\n",
    "    \n",
    "    words = list(set([word.lower() for sentence in df['comment'] for word in sentence.split()])) # unique words\n",
    "\n",
    "    #  dictionary:  word frequencies for each sentence\n",
    "    freq_dict = {}\n",
    "    for i, sentence in enumerate(df['comment']):\n",
    "        freq_dict[i] = {}\n",
    "        for word in sentence.split():\n",
    "            word = word.lower()\n",
    "            if word in freq_dict[i]:\n",
    "                freq_dict[i][word] += 1\n",
    "            else:\n",
    "                freq_dict[i][word] = 1\n",
    "\n",
    "    \n",
    "    matrix = [] # |comment| * |V|\n",
    "    for i in range(len(df)):\n",
    "        row = []\n",
    "        for word in words:\n",
    "            if word in freq_dict[i]:\n",
    "                row.append(freq_dict[i][word])\n",
    "            else:\n",
    "                row.append(0)\n",
    "        matrix.append(row)\n",
    "    matrix_df = pd.DataFrame(matrix, columns=words)\n",
    "    \n",
    "    matrixVV = np.dot(matrix_df.T,matrix_df) # |V|*|V|\n",
    "    total_words = sum(sum(matrixVV))\n",
    "    \n",
    "    matrixVV = matrixVV/total_words # to reach possibility\n",
    "    matrix_pmi = matrixVV # |V|*|V|\n",
    "\n",
    "\n",
    "    \n",
    "    row_sums = np.sum(matrixVV, axis=1) # sum of each row \n",
    "    col_sums = np.sum(matrixVV, axis=0) # sum of each column\n",
    "\n",
    "    matrixVV_with_row_sums = np.append(matrixVV, np.atleast_2d(row_sums).T, axis=1) # add to MatrixVV\n",
    "    col_sums = np.append(col_sums, np.sum(row_sums))\n",
    "    col_sums_2d = np.reshape(col_sums, (1, col_sums.shape[0]))\n",
    "    matrixVV_with_row_sums = np.append(matrixVV_with_row_sums, col_sums_2d, axis=0)\n",
    "\n",
    "\n",
    "    # print the result\n",
    "    # print(matrixVV_with_row_sums)\n",
    "\n",
    "    return matrixVV_with_row_sums, matrix_pmi, words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fd9cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, words = get_ppmi(preprocessed_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f99ca2b4",
   "metadata": {},
   "source": [
    "calulate pmi for each word in vocab\n",
    "\n",
    "p(t1,t2)/(p(t1)*p(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27435ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>تضمین</th>\n",
       "      <th>ساخت</th>\n",
       "      <th>کلاخمیربود</th>\n",
       "      <th>صداقتتون</th>\n",
       "      <th>خورمش</th>\n",
       "      <th>بیکران</th>\n",
       "      <th>نارنجک</th>\n",
       "      <th>کارکنانش</th>\n",
       "      <th>شدت</th>\n",
       "      <th>گلابی</th>\n",
       "      <th>...</th>\n",
       "      <th>چهارتادونه</th>\n",
       "      <th>ممنونممم</th>\n",
       "      <th>دونفرسیرمیکنه</th>\n",
       "      <th>خشکه</th>\n",
       "      <th>بیکن</th>\n",
       "      <th>ناکافی</th>\n",
       "      <th>زغال</th>\n",
       "      <th>انجا</th>\n",
       "      <th>پرانتز</th>\n",
       "      <th>قیمتا</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>تضمین</th>\n",
       "      <td>10.921983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ساخت</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.512487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>کلاخمیربود</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.149394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>صداقتتون</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.416457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>خورمش</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.028145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ناکافی</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.338464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>زغال</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.971618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>انجا</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.724758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>پرانتز</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.937845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.898781</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>قیمتا</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6254 rows × 6254 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                تضمین      ساخت  کلاخمیربود  صداقتتون     خورمش  بیکران   \n",
       "تضمین       10.921983  0.000000    0.000000  0.000000  0.000000     0.0  \\\n",
       "ساخت         0.000000  7.512487    0.000000  0.000000  0.000000     0.0   \n",
       "کلاخمیربود   0.000000  0.000000    8.149394  0.000000  0.000000     0.0   \n",
       "صداقتتون     0.000000  0.000000    0.000000  8.416457  0.000000     0.0   \n",
       "خورمش        0.000000  0.000000    0.000000  0.000000  8.028145     0.0   \n",
       "...               ...       ...         ...       ...       ...     ...   \n",
       "ناکافی       0.000000  0.000000    0.000000  0.000000  0.000000     0.0   \n",
       "زغال         0.000000  0.000000    0.000000  0.000000  0.000000     0.0   \n",
       "انجا         0.000000  0.000000    0.000000  0.000000  0.000000     0.0   \n",
       "پرانتز       0.000000  0.000000    0.000000  0.000000  0.000000     0.0   \n",
       "قیمتا        0.000000  0.000000    0.000000  0.000000  0.000000     0.0   \n",
       "\n",
       "            نارنجک  کارکنانش       شدت  گلابی  ...  چهارتادونه  ممنونممم   \n",
       "تضمین          0.0       0.0  0.000000    0.0  ...         0.0       0.0  \\\n",
       "ساخت           0.0       0.0  0.000000    0.0  ...         0.0       0.0   \n",
       "کلاخمیربود     0.0       0.0  0.000000    0.0  ...         0.0       0.0   \n",
       "صداقتتون       0.0       0.0  0.000000    0.0  ...         0.0       0.0   \n",
       "خورمش          0.0       0.0  0.000000    0.0  ...         0.0       0.0   \n",
       "...            ...       ...       ...    ...  ...         ...       ...   \n",
       "ناکافی         0.0       0.0  0.000000    0.0  ...         0.0       0.0   \n",
       "زغال           0.0       0.0  0.000000    0.0  ...         0.0       0.0   \n",
       "انجا           0.0       0.0  0.000000    0.0  ...         0.0       0.0   \n",
       "پرانتز         0.0       0.0  3.937845    0.0  ...         0.0       0.0   \n",
       "قیمتا          0.0       0.0  0.000000    0.0  ...         0.0       0.0   \n",
       "\n",
       "            دونفرسیرمیکنه  خشکه  بیکن    ناکافی      زغال      انجا    پرانتز   \n",
       "تضمین                 0.0   0.0   0.0  0.000000  0.000000  0.000000  0.000000  \\\n",
       "ساخت                  0.0   0.0   0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "کلاخمیربود            0.0   0.0   0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "صداقتتون              0.0   0.0   0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "خورمش                 0.0   0.0   0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "...                   ...   ...   ...       ...       ...       ...       ...   \n",
       "ناکافی                0.0   0.0   0.0  7.338464  0.000000  0.000000  0.000000   \n",
       "زغال                  0.0   0.0   0.0  0.000000  8.971618  0.000000  0.000000   \n",
       "انجا                  0.0   0.0   0.0  0.000000  0.000000  8.724758  0.000000   \n",
       "پرانتز                0.0   0.0   0.0  0.000000  0.000000  0.000000  8.898781   \n",
       "قیمتا                 0.0   0.0   0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "               قیمتا  \n",
       "تضمین       0.000000  \n",
       "ساخت        0.000000  \n",
       "کلاخمیربود  0.000000  \n",
       "صداقتتون    0.000000  \n",
       "خورمش       0.000000  \n",
       "...              ...  \n",
       "ناکافی      0.000000  \n",
       "زغال        0.000000  \n",
       "انجا        0.000000  \n",
       "پرانتز      0.000000  \n",
       "قیمتا       0.000006  \n",
       "\n",
       "[6254 rows x 6254 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0,x.shape[0]-1):\n",
    "    for j in range(0,y.shape[1]-1):\n",
    "        res = x[i][j]/(x[i][-1]*x[-1][j])\n",
    "        if res !=0:\n",
    "            pmi = np.log(res)\n",
    "            #print(pmi)\n",
    "            if pmi>0:\n",
    "                y[i][j] = pmi\n",
    "            else:\n",
    "                y[i][j] = 0\n",
    "        else:\n",
    "            y[i][j] = 0\n",
    "    \n",
    "ppmi_df = pd.DataFrame(y, columns=words, index=words)\n",
    "ppmi_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b93b6a8",
   "metadata": {},
   "source": [
    "create a vector for each vocab for each comment by mean on vectors of words of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42ef5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmi(df, ppmi_df):\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    for index, sen in df.iterrows():\n",
    "        text = sen['comment']\n",
    "    \n",
    "        tokens = hazm.word_tokenize(text)\n",
    "        n = len(tokens)\n",
    "        l = []\n",
    "        for word in tokens:\n",
    "            xx = ppmi_df[str(word)]\n",
    "            xx = xx.values\n",
    "            l.append(xx)\n",
    "        \n",
    "        l = sum(l)/n\n",
    "        l = l.reshape(1, -1)\n",
    "        ll = pd.DataFrame(l, columns=words)\n",
    "        ll[\"label_id\"] = sen['label_id']\n",
    "        result = pd.concat([result,ll], axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3485c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b94a11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm\n",
    "\n",
    "df_ppmi = get_pmi(preprocessed_df, ppmi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6b761fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppmi\n",
    "sum(df_ppmi.iloc[13])\n",
    "y = df_ppmi.label_id\n",
    "X = df_ppmi.drop('label_id', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d5cd675",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67852b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.61      0.72       359\n",
      "           1       0.68      0.91      0.78       331\n",
      "\n",
      "    accuracy                           0.76       690\n",
      "   macro avg       0.78      0.76      0.75       690\n",
      "weighted avg       0.79      0.76      0.75       690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa265c59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
